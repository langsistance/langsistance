# AgenticSeek: Alternativa Privada e Local ao Manus.

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

    English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md)

*Uma **alternativa 100% local ao Manus AI**, este assistente de voz com IA navega autonomamente na web, escreve c√≥digo e planeja tarefas mantendo todos os dados no seu dispositivo. Feito para modelos de racioc√≠nio locais, roda inteiramente no seu hardware, garantindo total privacidade e zero depend√™ncia da nuvem.*

[![Visite AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![Licen√ßa](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Por que AgenticSeek?

* üîí Totalmente Local & Privado - Tudo roda na sua m√°quina ‚Äî sem nuvem, sem compartilhamento de dados. Seus arquivos, conversas e buscas permanecem privados.

* üåê Navega√ß√£o Inteligente na Web - O AgenticSeek pode navegar na internet sozinho ‚Äî pesquisar, ler, extrair informa√ß√µes, preencher formul√°rios ‚Äî tudo sem as m√£os.

* üíª Assistente Aut√¥nomo de Programa√ß√£o - Precisa de c√≥digo? Ele pode escrever, depurar e executar programas em Python, C, Go, Java e mais ‚Äî tudo sem supervis√£o.

* üß† Sele√ß√£o Inteligente de Agentes - Voc√™ pede, ele escolhe automaticamente o melhor agente para a tarefa. Como ter uma equipe de especialistas pronta para ajudar.

* üìã Planeja & Executa Tarefas Complexas - De planejamento de viagens a projetos complexos ‚Äî pode dividir grandes tarefas em etapas e conclu√≠-las usando m√∫ltiplos agentes de IA.

* üéôÔ∏è Ativado por Voz - Voz limpa, r√°pida e futurista, al√©m de reconhecimento de fala, permitindo que voc√™ converse como se fosse sua IA pessoal de um filme de fic√ß√£o cient√≠fica. (Em desenvolvimento)

### **Demo**

> *Voc√™ pode pesquisar sobre o projeto agenticSeek, aprender quais habilidades s√£o necess√°rias, depois abrir o CV_candidates.zip e ent√£o me dizer quais combinam melhor com o projeto?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Aviso: Esta demonstra√ß√£o, incluindo todos os arquivos que aparecem (ex: CV_candidates.zip), s√£o totalmente fict√≠cios. N√£o somos uma corpora√ß√£o, buscamos colaboradores open-source, n√£o candidatos.

> üõ†‚ö†Ô∏èÔ∏è **Trabalho Ativo em Progresso**

> üôè Este projeto come√ßou como um projeto paralelo e n√£o tem roteiro nem financiamento. Cresceu muito al√©m do esperado ao aparecer no GitHub Trending. Contribui√ß√µes, feedback e paci√™ncia s√£o profundamente apreciados.

## Pr√©-requisitos

Certifique-se de ter chrome driver, docker e python3.10 instalados.

Para problemas relacionados ao chrome driver, veja a se√ß√£o **Chromedriver**.

### 1. **Clone o reposit√≥rio e configure**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Altere o conte√∫do do arquivo .env

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='opcional'
DEEPSEEK_API_KEY='opcional'
OPENROUTER_API_KEY='opcional'
TOGETHER_API_KEY='opcional'
GOOGLE_API_KEY='opcional'
ANTHROPIC_API_KEY='opcional'
```

**As chaves de API s√£o totalmente opcionais para quem optar por rodar LLM localmente. Que √© o objetivo principal deste projeto. Deixe em branco se voc√™ tiver hardware suficiente**

As seguintes vari√°veis de ambiente configuram as conex√µes e chaves de API do seu aplicativo.

Atualize o arquivo `.env` com seus pr√≥prios valores conforme necess√°rio:

- **SEARXNG_BASE_URL**: Deixe inalterado
- **REDIS_BASE_URL**: Deixe inalterado
- **WORK_DIR**: Caminho para seu diret√≥rio de trabalho local. O AgenticSeek poder√° ler e interagir com esses arquivos.
- **OLLAMA_PORT**: Porta para o servi√ßo Ollama.
- **LM_STUDIO_PORT**: Porta para o servi√ßo LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Porta para qualquer servi√ßo LLM adicional.

Todas as vari√°veis de ambiente de chave de API abaixo s√£o **opcionais**. S√≥ forne√ßa se for usar APIs externas em vez de rodar LLMs localmente.

### 3. **Inicie o Docker**

Certifique-se de que o Docker est√° instalado e rodando no seu sistema. Voc√™ pode iniciar o Docker com os seguintes comandos:

- **No Linux/macOS:**  
        Abra um terminal e execute:
        ```sh
        sudo systemctl start docker
        ```
        Ou inicie o Docker Desktop pelo menu de aplicativos, se instalado.

- **No Windows:**  
        Inicie o Docker Desktop pelo menu Iniciar.

Voc√™ pode verificar se o Docker est√° rodando executando:
```sh
docker info
```
Se aparecerem informa√ß√µes sobre sua instala√ß√£o do Docker, est√° funcionando corretamente.

---

## Configura√ß√£o para rodar LLM localmente na sua m√°quina

**Requisitos de Hardware:**

Para rodar LLMs localmente, voc√™ precisar√° de hardware suficiente. No m√≠nimo, uma GPU capaz de rodar Qwen/Deepseek 14B √© necess√°ria. Veja o FAQ para recomenda√ß√µes detalhadas de modelo/desempenho.

**Configure seu provedor local**

Inicie seu provedor local, por exemplo com ollama:

```sh
ollama serve
```

Veja abaixo a lista de provedores locais suportados.

**Atualize o config.ini**

Altere o arquivo config.ini para definir o provider_name para um provedor suportado e provider_model para um LLM suportado pelo seu provedor. Recomendamos modelos de racioc√≠nio como *Qwen* ou *Deepseek*.

Veja o **FAQ** no final do README para hardware necess√°rio.

```sh
[MAIN]
is_local = True # Se est√° rodando localmente ou com provedor remoto.
provider_name = ollama # ou lm-studio, openai, etc.
provider_model = deepseek-r1:14b # escolha um modelo compat√≠vel com seu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nome da sua IA
recover_last_session = True # recuperar sess√£o anterior
save_session = True # lembrar sess√£o atual
speak = False # texto para fala
listen = False # fala para texto, apenas para CLI, experimental
jarvis_personality = False # usar personalidade "Jarvis" (experimental)
languages = en zh # Lista de idiomas, TTS usar√° o primeiro da lista
[BROWSER]
headless_browser = True # deixe inalterado a menos que use CLI no host.
stealth_mode = True # Usa selenium indetect√°vel para reduzir detec√ß√£o do navegador
```

**Aviso**:

- O formato do arquivo `config.ini` n√£o suporta coment√°rios.
N√£o copie e cole a configura√ß√£o de exemplo diretamente, pois coment√°rios causar√£o erros. Em vez disso, modifique manualmente o arquivo `config.ini` com suas configura√ß√µes desejadas, sem coment√°rios.

- *N√ÉO* defina provider_name como `openai` se estiver usando LM-studio para rodar LLMs. Use `lm-studio`.

- Alguns provedores (ex: lm-studio) exigem `http://` antes do IP. Exemplo: `http://127.0.0.1:1234`

**Lista de provedores locais**

| Provedor   | Local? | Descri√ß√£o                                               |
|------------|--------|---------------------------------------------------------|
| ollama     | Sim    | Rode LLMs localmente facilmente usando ollama           |
| lm-studio  | Sim    | Rode LLM localmente com LM studio (`provider_name` = `lm-studio`)|
| openai     | Sim    | Use API compat√≠vel com openai (ex: servidor llama.cpp)  |

Pr√≥ximo passo: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)

*Veja a se√ß√£o **Problemas conhecidos** se tiver problemas*

*Veja a se√ß√£o **Rodar com uma API** se seu hardware n√£o rodar deepseek localmente*

*Veja a se√ß√£o **Config** para explica√ß√£o detalhada do arquivo de configura√ß√£o.*

---

## Configura√ß√£o para rodar com uma API

**Rodar com uma API √© opcional, veja acima para rodar localmente.**

Defina o provedor desejado no `config.ini`. Veja abaixo a lista de provedores de API.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # n√£o importa
```
Aviso: Certifique-se de n√£o haver espa√ßo no final da linha no config.

Exporte sua chave de API: `export <<PROVIDER>>_API_KEY="xxx"`

Exemplo: exportar `TOGETHER_API_KEY="xxxxx"`

**Lista de provedores de API**
    
| Provedor   | Local? | Descri√ß√£o                                               |
|------------|--------|---------------------------------------------------------|
| openai     | Depende| Use API do ChatGPT  |
| deepseek   | N√£o    | API Deepseek (n√£o privado)                              |
| huggingface| N√£o    | API Hugging-Face (n√£o privado)                          |
| togetherAI | N√£o    | Use API together AI (n√£o privado)                       |
| google     | N√£o    | Use API google gemini (n√£o privado)                     |

Observe que c√≥digo/bash pode falhar com gemini, pois ignora nosso prompt de formata√ß√£o, que √© otimizado para deepseek r1. Modelos como gpt-4o tamb√©m apresentam desempenho ruim com nosso prompt.

Pr√≥ximo passo: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)

*Veja a se√ß√£o **Problemas conhecidos** se tiver problemas*

*Veja a se√ß√£o **Config** para explica√ß√£o detalhada do arquivo de configura√ß√£o.*

---

## Inicie os servi√ßos e rode

Inicie os servi√ßos necess√°rios. Isso iniciar√° todos os servi√ßos do docker-compose.yml, incluindo:
        - searxng
        - redis (necess√°rio para searxng)
        - frontend
        - backend (se usar `full`)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Windows
```

**Aviso:** Este passo far√° download e carregar√° todas as imagens Docker, o que pode levar at√© 30 minutos. Ap√≥s iniciar os servi√ßos, aguarde at√© que o servi√ßo backend esteja totalmente rodando (voc√™ ver√° backend: <info> no log) antes de enviar mensagens. O backend pode demorar mais para iniciar.

Acesse `http://localhost:3000/` e voc√™ ver√° a interface web.

**Opcional:** Rode com a interface CLI:

Para rodar com CLI, instale os pacotes no host:

```sh
./install.sh
./install.bat # windows
```

Inicie os servi√ßos:

```sh
./start_services.sh # MacOS
start start_services.cmd # Windows
```

Depois execute: `uv run cli.py`

---

## Uso

Certifique-se de que os servi√ßos est√£o rodando com `./start_services.sh full` e acesse `localhost:3000` para a interface web.

Voc√™ tamb√©m pode usar fala para texto definindo `listen = True` no config. Apenas para modo CLI.

Para sair, basta dizer/digitar `goodbye`.

Exemplos de uso:

> *Fa√ßa um jogo da cobrinha em python!*

> *Pesquise na web pelos melhores caf√©s em Rennes, Fran√ßa, e salve uma lista de tr√™s com seus endere√ßos em rennes_cafes.txt.*

> *Escreva um programa Go para calcular o fatorial de um n√∫mero, salve como factorial.go no seu workspace*

> *Procure na pasta summer_pictures por todos os arquivos JPG, renomeie com a data de hoje e salve a lista dos arquivos renomeados em photos_list.txt*

> *Pesquise online por filmes de fic√ß√£o cient√≠fica populares de 2024 e escolha tr√™s para assistir hoje √† noite. Salve a lista em movie_night.txt.*

> *Pesquise na web pelos √∫ltimos artigos de not√≠cias de IA de 2025, selecione tr√™s e escreva um script Python para extrair t√≠tulos e resumos. Salve o script como news_scraper.py e os resumos em ai_news.txt em /home/projects*

> *Sexta-feira, pesquise na web por uma API gratuita de pre√ßos de a√ß√µes, registre-se com supersuper7434567@gmail.com e escreva um script Python para buscar os pre√ßos di√°rios da Tesla usando a API, salvando os resultados em stock_prices.csv*

*Observe que o preenchimento de formul√°rios ainda √© experimental e pode falhar.*

Ap√≥s digitar sua consulta, o AgenticSeek alocar√° o melhor agente para a tarefa.

Como este √© um prot√≥tipo inicial, o sistema de roteamento de agentes pode n√£o alocar sempre o agente certo para sua consulta.

Portanto, seja expl√≠cito no que deseja e como a IA deve proceder. Por exemplo, se quiser que fa√ßa uma busca na web, n√£o diga:

`Voc√™ conhece alguns bons pa√≠ses para viajar sozinho?`

Em vez disso, pe√ßa:

`Fa√ßa uma busca na web e descubra quais s√£o os melhores pa√≠ses para viajar sozinho`

---

## **Configura√ß√£o para rodar o LLM em seu pr√≥prio servidor**

Se voc√™ tem um computador potente ou servidor, mas quer usar a partir do seu laptop, pode rodar o LLM em um servidor remoto usando nosso servidor LLM customizado.

No seu "servidor" que rodar√° o modelo de IA, obtenha o endere√ßo IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip local
curl https://ipinfo.io/ip # ip p√∫blico
```

Nota: Para Windows ou macOS, use ipconfig ou ifconfig para encontrar o IP.

Clone o reposit√≥rio e entre na pasta `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Instale os requisitos espec√≠ficos do servidor:

```sh
pip3 install -r requirements.txt
```

Rode o script do servidor.

```sh
python3 app.py --provider ollama --port 3333
```

Voc√™ pode escolher entre usar `ollama` e `llamacpp` como servi√ßo LLM.

Agora, no seu computador pessoal:

Altere o arquivo `config.ini` para definir `provider_name` como `server` e `provider_model` como `deepseek-r1:xxb`.
Defina `provider_server_address` para o IP da m√°quina que rodar√° o modelo.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```

Pr√≥ximo passo: [Inicie os servi√ßos e rode o AgenticSeek](#Start-services-and-Run)

---

## Fala para Texto

Aviso: fala para texto s√≥ funciona no modo CLI no momento.

Atualmente, fala para texto s√≥ funciona em ingl√™s.

A funcionalidade de fala para texto est√° desativada por padr√£o. Para ativar, defina listen como True no arquivo config.ini:

```
listen = True
```

Quando ativado, o recurso escuta por uma palavra-chave de ativa√ß√£o, que √© o nome do agente, antes de processar sua entrada. Voc√™ pode personalizar o nome do agente atualizando o valor `agent_name` no *config.ini*:

```
agent_name = Friday
```

Para melhor reconhecimento, recomendamos usar um nome comum em ingl√™s como "John" ou "Emma" como nome do agente.

Quando o transcript come√ßar a aparecer, diga o nome do agente em voz alta para ativ√°-lo (ex: "Friday").

Fale sua consulta claramente.

Finalize seu pedido com uma frase de confirma√ß√£o para o sistema prosseguir. Exemplos de frases de confirma√ß√£o incluem:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Configura√ß√£o

Exemplo de config:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explica√ß√£o**:

- is_local -> Roda o agente localmente (True) ou em servidor remoto (False).

- provider_name -> Provedor a ser usado (um de: `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -> Modelo usado, ex: deepseek-r1:32b.

- provider_server_address -> Endere√ßo do servidor, ex: 127.0.0.1:11434 para local. Qualquer valor para API n√£o local.

- agent_name -> Nome do agente, ex: Friday. Usado como palavra-chave para TTS.

- recover_last_session -> Retoma da √∫ltima sess√£o (True) ou n√£o (False).

- save_session -> Salva dados da sess√£o (True) ou n√£o (False).

- speak -> Ativa sa√≠da de voz (True) ou n√£o (False).

- listen -> Ativa entrada por voz (True) ou n√£o (False).

- jarvis_personality -> Usa personalidade tipo JARVIS (True) ou n√£o (False). Apenas muda o prompt.

- languages -> Lista de idiomas suportados, necess√°rio para o roteador de LLM funcionar corretamente. Evite muitos idiomas ou muito parecidos.

- headless_browser -> Roda navegador sem janela vis√≠vel (True) ou n√£o (False).

- stealth_mode -> Dificulta detec√ß√£o de bot. √önico contra √© instalar manualmente a extens√£o anticaptcha.

- languages -> Lista de idiomas suportados. Necess√°rio para o sistema de roteamento de agentes. Quanto maior a lista, mais modelos ser√£o baixados.

## Provedores

Tabela de provedores dispon√≠veis:

| Provedor   | Local? | Descri√ß√£o                                               |
|------------|--------|---------------------------------------------------------|
| ollama     | Sim    | Rode LLMs localmente facilmente usando ollama           |
| server     | Sim    | Hospede o modelo em outra m√°quina, use localmente       |
| lm-studio  | Sim    | Rode LLM localmente com LM studio (`lm-studio`)         |
| openai     | Depende| Use API do ChatGPT (n√£o privado) ou API compat√≠vel      |
| deepseek-api| N√£o   | API Deepseek (n√£o privado)                              |
| huggingface| N√£o    | API Hugging-Face (n√£o privado)                          |
| togetherAI | N√£o    | Use API together AI (n√£o privado)                       |
| google     | N√£o    | Use API google gemini (n√£o privado)                     |

Para selecionar um provedor, altere o config.ini:

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local`: deve ser True para qualquer LLM rodando localmente, sen√£o False.

`provider_name`: Selecione o provedor pelo nome, veja a lista acima.

`provider_model`: Defina o modelo a ser usado pelo agente.

`provider_server_address`: pode ser qualquer valor se n√£o usar o provedor server.

# Problemas conhecidos

## Problemas com Chromedriver

**Erro conhecido #1:** *chromedriver incompat√≠vel*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Isso ocorre se houver incompatibilidade entre seu navegador e a vers√£o do chromedriver.

Voc√™ precisa baixar a vers√£o mais recente:

https://developer.chrome.com/docs/chromedriver/downloads

Se estiver usando Chrome vers√£o 115 ou superior, acesse:

https://googlechromelabs.github.io/chrome-for-testing/

E baixe o chromedriver correspondente ao seu sistema operacional.

![alt text](./media/chromedriver_readme.png)

Se esta se√ß√£o estiver incompleta, abra uma issue.

## Problemas de adaptadores de conex√£o

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:11434/v1/chat/completions'
```

Certifique-se de ter `http://` antes do IP do provedor:

`provider_server_address = http://127.0.0.1:11434`

## SearxNG base URL deve ser fornecida

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

Talvez voc√™ n√£o tenha movido `.env.example` para `.env`? Voc√™ tamb√©m pode exportar SEARXNG_BASE_URL:

`export  SEARXNG_BASE_URL="http://127.0.0.1:8080"`

## FAQ

**P: Que hardware eu preciso?**  

| Tamanho do Modelo | GPU         | Coment√°rio                                               |
|-------------------|-------------|---------------------------------------------------------|
| 7B                | 8GB Vram    | ‚ö†Ô∏è N√£o recomendado. Desempenho ruim, alucina√ß√µes frequentes, agentes de planejamento podem falhar. |
| 14B               | 12 GB VRAM (ex: RTX 3060) | ‚úÖ Us√°vel para tarefas simples. Pode ter dificuldades com navega√ß√£o web e planejamento. |
| 32B               | 24+ GB VRAM (ex: RTX 4090) | üöÄ Sucesso na maioria das tarefas, pode ainda ter dificuldades com planejamento |
| 70B+              | 48+ GB Vram (ex: mac studio) | üí™ Excelente. Recomendado para uso avan√ßado. |

**P: Por que Deepseek R1 em vez de outros modelos?**  

Deepseek R1 se destaca em racioc√≠nio e uso de ferramentas para seu tamanho. Achamos que √© uma √≥tima escolha para nossas necessidades, outros modelos funcionam bem, mas Deepseek √© nossa principal escolha.

**P: Recebo erro ao rodar `cli.py`. O que fa√ßo?**  

Certifique-se de que o local est√° rodando (`ollama serve`), seu `config.ini` corresponde ao provedor e as depend√™ncias est√£o instaladas. Se nada funcionar, abra uma issue.

**P: Pode rodar 100% localmente mesmo?**  

Sim, com Ollama, lm-studio ou provedores server, todo o reconhecimento de fala, LLM e TTS rodam localmente. Op√ß√µes n√£o locais (OpenAI ou outras APIs) s√£o opcionais.

**P: Por que usar AgenticSeek se j√° tenho Manus?**

Come√ßou como um projeto paralelo por interesse em agentes de IA. O diferencial √© usar modelos locais e evitar APIs.
Nos inspiramos em Jarvis e Friday (filmes do Homem de Ferro) para torn√°-lo "legal", mas funcionalmente nos inspiramos mais no Manus, pois √© isso que as pessoas querem: uma alternativa local ao Manus.
Ao contr√°rio do Manus, o AgenticSeek prioriza independ√™ncia de sistemas externos, dando mais controle, privacidade e evitando custos de API.

## Contribua

Procuramos desenvolvedores para melhorar o AgenticSeek! Veja as issues abertas ou discuss√µes.

[Guia de contribui√ß√£o](./docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Mantenedores:

 > [Fosowl](https://github.com/Fosowl) | Hor√°rio de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Hor√°rio de Taipei 

 > [steveh8758](https://github.com/steveh8758) | Hor√°rio de Taipei 

## Agradecimentos Especiais:

 > [tcsenpai](https://github.com/tcsenpai) e [plitc](https://github.com/plitc) pela ajuda na dockeriza√ß√£o do backend

