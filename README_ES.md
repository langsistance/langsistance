# AgenticSeek: Alternativa Privada y Local a Manus.

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  [English](./README.md) | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md) | [Espa√±ol](./README_ES.md)

*Una **alternativa 100% local a Manus AI**, este asistente de IA con capacidad de voz navega aut√≥nomamente por la web, escribe c√≥digo y planifica tareas mientras mantiene todos los datos en tu dispositivo. Dise√±ado para modelos de razonamiento local, funciona completamente en tu hardware, garantizando privacidad total y cero dependencia de la nube.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### ¬øPor qu√© AgenticSeek?

* üîí Totalmente Local y Privado - Todo se ejecuta en tu m√°quina ‚Äî sin nube, sin compartir datos. Tus archivos, conversaciones y b√∫squedas permanecen privados.

* üåê Navegaci√≥n Web Inteligente - AgenticSeek puede navegar por internet por s√≠ mismo ‚Äî buscar, leer, extraer informaci√≥n, llenar formularios web ‚Äî todo sin intervenci√≥n manual.

* üíª Asistente de C√≥digo Aut√≥nomo - ¬øNecesitas c√≥digo? Puede escribir, depurar y ejecutar programas en Python, C, Go, Java y m√°s ‚Äî todo sin supervisi√≥n.

* üß† Selecci√≥n Inteligente de Agentes - T√∫ preguntas, √©l determina el mejor agente para el trabajo autom√°ticamente. Como tener un equipo de expertos listos para ayudar.

* üìã Planifica y Ejecuta Tareas Complejas - Desde planificaci√≥n de viajes hasta proyectos complejos ‚Äî puede dividir grandes tareas en pasos y completarlas usando m√∫ltiples agentes de IA.

* üéôÔ∏è Habilitado por Voz - Voz y texto a voz limpios, r√°pidos y futuristas que te permiten hablar con √©l como si fuera tu IA personal de una pel√≠cula de ciencia ficci√≥n. (En progreso)

### **Demo**

> *¬øPuedes buscar el proyecto agenticSeek, aprender qu√© habilidades se requieren, luego abrir el CV_candidates.zip y decirme cu√°les coinciden mejor con el proyecto?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Descargo de responsabilidad: Esta demo, incluyendo todos los archivos que aparecen (ej: CV_candidates.zip), son completamente ficticios. No somos una corporaci√≥n, buscamos contribuidores de c√≥digo abierto, no candidatos.

> üõ†‚ö†Ô∏èÔ∏è **Trabajo Activo en Progreso**

> üôè Este proyecto comenz√≥ como un proyecto secundario y no tiene hoja de ruta ni financiamiento. Ha crecido mucho m√°s all√° de lo que esperaba al terminar en GitHub Trending. Las contribuciones, comentarios y paciencia son profundamente apreciados.

## Prerrequisitos

Aseg√∫rate de tener instalado chrome driver, docker y python3.10.

Para problemas relacionados con chrome driver, consulta la secci√≥n **Chromedriver**.

### 1. **Clonar el repositorio y configuraci√≥n**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Cambiar el contenido del archivo .env

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='opcional'
DEEPSEEK_API_KEY='opcional'
OPENROUTER_API_KEY='opcional'
TOGETHER_API_KEY='opcional'
GOOGLE_API_KEY='opcional'
ANTHROPIC_API_KEY='opcional'
```

**Las API Keys son totalmente opcionales para usuarios que elijan ejecutar LLM localmente. Este es el prop√≥sito principal de este proyecto. D√©jalas vac√≠as si tienes hardware suficiente**

Las siguientes variables de entorno configuran las conexiones y claves API de tu aplicaci√≥n.

Actualiza el archivo `.env` con tus propios valores seg√∫n sea necesario:

- **SEARXNG_BASE_URL**: D√©jalo sin cambios
- **REDIS_BASE_URL**: D√©jalo sin cambios
- **WORK_DIR**: Ruta a tu directorio de trabajo en tu m√°quina local. AgenticSeek podr√° leer e interactuar con estos archivos.
- **OLLAMA_PORT**: N√∫mero de puerto para el servicio Ollama.
- **LM_STUDIO_PORT**: N√∫mero de puerto para el servicio LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Puerto para cualquier servicio LLM personalizado adicional.

Todas las variables de entorno de API key a continuaci√≥n son **opcionales**. Solo necesitas proporcionarlas si planeas usar APIs externas en lugar de ejecutar LLMs localmente.

### 3. **Iniciar Docker**

Aseg√∫rate de que Docker est√© instalado y ejecut√°ndose en tu sistema. Puedes iniciar Docker usando los siguientes comandos:

- **En Linux/macOS:**  
    Abre una terminal y ejecuta:
    ```sh
    sudo systemctl start docker
    ```
    O inicia Docker Desktop desde el men√∫ de aplicaciones si est√° instalado.

- **En Windows:**  
    Inicia Docker Desktop desde el men√∫ de inicio.

Puedes verificar que Docker est√° ejecut√°ndose ejecutando:
```sh
docker info
```
Si ves informaci√≥n sobre tu instalaci√≥n de Docker, est√° ejecut√°ndose correctamente.

---

## Configuraci√≥n para ejecutar LLM localmente en tu m√°quina

**Requisitos de Hardware:**

Para ejecutar LLMs localmente, necesitar√°s hardware suficiente. Como m√≠nimo, se requiere una GPU capaz de ejecutar Qwen/Deepseek 14B. Consulta la secci√≥n FAQ para recomendaciones detalladas de modelos/rendimiento.

**Configura tu proveedor local**  

Inicia tu proveedor local, por ejemplo con ollama:

```sh
ollama serve
```

Consulta m√°s abajo la lista de proveedores locales soportados.

**Actualiza el config.ini**

Cambia el archivo config.ini para establecer provider_name a un proveedor soportado y provider_model a un LLM soportado por tu proveedor. Recomendamos modelos de razonamiento como *Qwen* o *Deepseek*.

Consulta la secci√≥n **FAQ** al final del README para los requisitos de hardware.

```sh
[MAIN]
is_local = True # Si est√°s ejecutando localmente o con proveedor remoto.
provider_name = ollama # o lm-studio, openai, etc..
provider_model = deepseek-r1:14b # elige un modelo que se ajuste a tu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nombre de tu IA
recover_last_session = True # si quieres recuperar la sesi√≥n anterior
save_session = True # si quieres recordar la sesi√≥n actual
speak = False # texto a voz
listen = False # Voz a texto, solo para CLI, experimental
jarvis_personality = False # Si quieres usar una personalidad m√°s tipo "Jarvis" (experimental)
languages = en zh # La lista de idiomas, Text to speech usar√° por defecto el primer idioma de la lista
[BROWSER]
headless_browser = True # d√©jalo sin cambios a menos que uses CLI en el host.
stealth_mode = True # Usa selenium no detectado para reducir la detecci√≥n del navegador
```

**Advertencia**:

- El formato del archivo `config.ini` no soporta comentarios.
No copies y pegues la configuraci√≥n de ejemplo directamente, ya que los comentarios causar√°n errores. En su lugar, modifica manualmente el archivo `config.ini` con tus configuraciones deseadas, excluyendo cualquier comentario.

- NO establezcas provider_name como `openai` si est√°s usando LM-studio para ejecutar LLMs. Establ√©celo como `lm-studio`.

- Algunos proveedores (ej: lm-studio) requieren que tengas `http://` antes de la IP. Por ejemplo `http://127.0.0.1:1234`

**Lista de proveedores locales**

| Proveedor  | ¬øLocal? | Descripci√≥n                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | S√≠    | Ejecuta LLMs localmente con facilidad usando ollama como proveedor LLM |
| lm-studio  | S√≠    | Ejecuta LLM localmente con LM studio (establece `provider_name` como `lm-studio`)|
| openai    | S√≠     |  Usa API compatible con openai (ej: servidor llama.cpp)  |

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#Iniciar-servicios-y-ejecutar)  

*Consulta la secci√≥n **Problemas conocidos** si tienes problemas*

*Consulta la secci√≥n **Ejecutar con una API** si tu hardware no puede ejecutar deepseek localmente*

*Consulta la secci√≥n **Config** para una explicaci√≥n detallada del archivo de configuraci√≥n.*

---

## Configuraci√≥n para ejecutar con una API

**Ejecutar con una API es opcional, consulta arriba para ejecutar localmente.**

Establece el proveedor deseado en el `config.ini`. Consulta m√°s abajo la lista de proveedores de API.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # no importa
```
Advertencia: Aseg√∫rate de que no haya espacios al final en la configuraci√≥n.

Exporta tu API key: `export <<PROVIDER>>_API_KEY="xxx"`

Ejemplo: export `TOGETHER_API_KEY="xxxxx"`

**Lista de proveedores de API**
  
| Proveedor  | ¬øLocal? | Descripci√≥n                                               |
|-----------|--------|-----------------------------------------------------------|
| openai    | Depende  | Usa API de ChatGPT  |
| deepseek  | No     | API de Deepseek (no privada)                            |
| huggingface| No    | API de Hugging-Face (no privada)                            |
| togetherAI | No    | Usa API de together AI (no privada)                         |
| google | No    | Usa API de google gemini (no privada)                         |

Ten en cuenta que la codificaci√≥n/bash podr√≠a fallar con gemini, parece ignorar nuestro prompt para respetar el formato, que est√°n optimizados para deepseek r1. Modelos como gpt-4 tambi√©n parecen tener un rendimiento deficiente con nuestro prompt.

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#Iniciar-servicios-y-ejecutar)

*Consulta la secci√≥n **Problemas conocidos** si tienes problemas*

*Consulta la secci√≥n **Config** para una explicaci√≥n detallada del archivo de configuraci√≥n.*

---

## Iniciar servicios y ejecutar

Inicia los servicios requeridos. Esto iniciar√° todos los servicios del docker-compose.yml, incluyendo:
    - searxng
    - redis (requerido por searxng)
    - frontend
    - backend (si usas `full`)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Windows
```

**Advertencia:** Este paso descargar√° y cargar√° todas las im√°genes de Docker, lo que puede tomar hasta 30 minutos. Despu√©s de iniciar los servicios, espera hasta que el servicio backend est√© completamente ejecut√°ndose (deber√≠as ver backend: <info> en el log) antes de enviar cualquier mensaje. Los servicios backend pueden tardar m√°s en iniciar que otros.

Ve a `http://localhost:3000/` y deber√≠as ver la interfaz web.

**Opcional:** Ejecutar con la interfaz CLI:

Para ejecutar con la interfaz CLI tendr√≠as que instalar el paquete en el host:

```sh
./install.sh
./install.bat # windows
```

Inicia los servicios:

```sh
./start_services.sh # MacOS
start start_services.cmd # Windows
```

Luego ejecuta: `uv run cli.py` 

## Uso

Aseg√∫rate de que los servicios est√©n funcionando con `./start_services.sh full` y ve a `localhost:3000` para la interfaz web.

Tambi√©n puedes usar el reconocimiento de voz configurando `listen = True` en el config. Solo para modo CLI.

Para salir, simplemente di/escribe `goodbye`.

Aqu√≠ hay algunos ejemplos de uso:

> *¬°Haz un juego de serpiente en python!*

> *Busca en la web los mejores caf√©s en Rennes, Francia, y guarda una lista de tres con sus direcciones en rennes_cafes.txt.*

> *Escribe un programa en Go para calcular el factorial de un n√∫mero, gu√°rdalo como factorial.go en tu espacio de trabajo*

> *Busca en mi carpeta summer_pictures todos los archivos JPG, ren√≥mbralos con la fecha de hoy, y guarda una lista de archivos renombrados en photos_list.txt*

> *Busca en l√≠nea pel√≠culas populares de ciencia ficci√≥n de 2024 y elige tres para ver esta noche. Guarda la lista en movie_night.txt.*

> *Busca en la web los √∫ltimos art√≠culos de noticias sobre IA de 2025, selecciona tres, y escribe un script de Python para extraer sus t√≠tulos y res√∫menes. Guarda el script como news_scraper.py y los res√∫menes en ai_news.txt en /home/projects*

> *Viernes, busca en la web una API gratuita de precios de acciones, reg√≠strate con supersuper7434567@gmail.com luego escribe un script de Python para obtener usando la API los precios diarios de Tesla, y guarda los resultados en stock_prices.csv*

*Ten en cuenta que las capacidades de llenado de formularios son a√∫n experimentales y podr√≠an fallar.*

Despu√©s de escribir tu consulta, AgenticSeek asignar√° el mejor agente para la tarea.

Debido a que este es un prototipo temprano, el sistema de enrutamiento de agentes podr√≠a no asignar siempre el agente correcto seg√∫n tu consulta.

Por lo tanto, debes ser muy expl√≠cito en lo que quieres y c√≥mo la IA podr√≠a proceder, por ejemplo, si quieres que realice una b√∫squeda web, no digas:

`¬øConoces algunos buenos pa√≠ses para viajar solo?`

En su lugar, pregunta:

`Haz una b√∫squeda web y averigua cu√°les son los mejores pa√≠ses para viajar solo`

---

## **Configuraci√≥n para ejecutar el LLM en tu propio servidor**

Si tienes una computadora potente o un servidor que puedes usar, pero quieres usarlo desde tu laptop, tienes la opci√≥n de ejecutar el LLM en un servidor remoto usando nuestro servidor llm personalizado.

En tu "servidor" que ejecutar√° el modelo de IA, obt√©n la direcci√≥n IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip local
curl https://ipinfo.io/ip # ip p√∫blica
```

Nota: Para Windows o macOS, usa ipconfig o ifconfig respectivamente para encontrar la direcci√≥n IP.

Clona el repositorio y entra en la carpeta `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Instala los requisitos espec√≠ficos del servidor:

```sh
pip3 install -r requirements.txt
```

Ejecuta el script del servidor.

```sh
python3 app.py --provider ollama --port 3333
```

Tienes la opci√≥n de usar `ollama` y `llamacpp` como servicio LLM.

Ahora en tu computadora personal:

Cambia el archivo `config.ini` para establecer `provider_name` a `server` y `provider_model` a `deepseek-r1:xxb`.
Establece `provider_server_address` a la direcci√≥n IP de la m√°quina que ejecutar√° el modelo.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#Iniciar-servicios-y-ejecutar)

---

## Voz a Texto

Advertencia: la voz a texto solo funciona en modo CLI por el momento.

Ten en cuenta que actualmente la voz a texto solo funciona en ingl√©s.

La funcionalidad de voz a texto est√° deshabilitada por defecto. Para habilitarla, establece la opci√≥n listen a True en el archivo config.ini:

```
listen = True
```

Cuando est√° habilitada, la funci√≥n de voz a texto escucha una palabra clave de activaci√≥n, que es el nombre del agente, antes de comenzar a procesar tu entrada. Puedes personalizar el nombre del agente actualizando el valor `agent_name` en el archivo *config.ini*:

```
agent_name = Friday
```

Para un reconocimiento √≥ptimo, recomendamos usar un nombre com√∫n en ingl√©s como "John" o "Emma" como nombre del agente.

Una vez que veas que comienza a aparecer la transcripci√≥n, di el nombre del agente en voz alta para activarlo (por ejemplo, "Friday").

Habla tu consulta claramente.

Termina tu solicitud con una frase de confirmaci√≥n para indicar al sistema que proceda. Ejemplos de frases de confirmaci√≥n incluyen:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Config

Ejemplo de configuraci√≥n:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explicaci√≥n**:

- is_local -> Ejecuta el agente localmente (True) o en un servidor remoto (False).

- provider_name -> El proveedor a usar (uno de: `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -> El modelo usado, ej., deepseek-r1:32b.

- provider_server_address -> Direcci√≥n del servidor, ej., 127.0.0.1:11434 para local. Establece a cualquier cosa para API no local.

- agent_name -> Nombre del agente, ej., Friday. Usado como palabra de activaci√≥n para TTS.

- recover_last_session -> Reinicia desde la √∫ltima sesi√≥n (True) o no (False).

- save_session -> Guarda datos de la sesi√≥n (True) o no (False).

- speak -> Habilita salida de voz (True) o no (False).

- listen -> escucha entrada de voz (True) o no (False).

- jarvis_personality -> Usa una personalidad tipo JARVIS (True) o no (False). Esto simplemente cambia el archivo de prompt.

- languages -> La lista de idiomas soportados, necesaria para que el router llm funcione correctamente, evita poner demasiados o idiomas demasiado similares.

- headless_browser -> Ejecuta el navegador sin una ventana visible (True) o no (False).

- stealth_mode -> Hace m√°s dif√≠cil la detecci√≥n de bots. El √∫nico inconveniente es que tienes que instalar manualmente la extensi√≥n anticaptcha.

- languages -> Lista de idiomas soportados. Requerido para el sistema de enrutamiento de agentes. Cuanto m√°s larga sea la lista de idiomas, m√°s modelos se descargar√°n.

## Proveedores

La tabla a continuaci√≥n muestra los proveedores disponibles:

| Proveedor  | ¬øLocal? | Descripci√≥n                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | S√≠    | Ejecuta LLMs localmente con facilidad usando ollama como proveedor LLM |
| server    | S√≠    | Aloja el modelo en otra m√°quina, ejecuta tu m√°quina local |
| lm-studio  | S√≠    | Ejecuta LLM localmente con LM studio (`lm-studio`)             |
| openai    | Depende  | Usa API de ChatGPT (no privada) o API compatible con openai  |
| deepseek-api  | No     | API de Deepseek (no privada)                            |
| huggingface| No    | API de Hugging-Face (no privada)                            |
| togetherAI | No    | Usa API de together AI (no privada)                         |
| google | No    | Usa API de google gemini (no privada)                         |

Para seleccionar un proveedor cambia el config.ini:

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local`: debe ser True para cualquier LLM ejecut√°ndose localmente, de lo contrario False.

`provider_name`: Selecciona el proveedor a usar por su nombre, ver la lista de proveedores arriba.

`provider_model`: Establece el modelo a usar por el agente.

`provider_server_address`: puede establecerse a cualquier cosa si no est√°s usando el proveedor server.

# Problemas conocidos

## Problemas con Chromedriver

**Error conocido #1:** *incompatibilidad de chromedriver*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Esto ocurre si hay una incompatibilidad entre tu navegador y la versi√≥n de chromedriver.

Necesitas navegar para descargar la √∫ltima versi√≥n:

https://developer.chrome.com/docs/chromedriver/downloads

Si est√°s usando Chrome versi√≥n 115 o m√°s nueva ve a:

https://googlechromelabs.github.io/chrome-for-testing/

Y descarga la versi√≥n de chromedriver que coincida con tu SO.

![alt text](./media/chromedriver_readme.png)

Si esta secci√≥n est√° incompleta por favor crea un issue.

## Problemas con adaptadores de conexi√≥n

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:11434/v1/chat/completions'
```

Aseg√∫rate de tener `http://` antes de la direcci√≥n IP del proveedor:

`provider_server_address = http://127.0.0.1:11434`

## Se debe proporcionar la URL base de SearxNG

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

¬øQuiz√°s no moviste `.env.example` como `.env`? Tambi√©n puedes exportar SEARXNG_BASE_URL: 

`export  SEARXNG_BASE_URL="http://127.0.0.1:8080"`

## FAQ

**P: ¬øQu√© hardware necesito?**  

| Tama√±o del Modelo  | GPU  | Comentario                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB Vram | ‚ö†Ô∏è No recomendado. El rendimiento es pobre, alucinaciones frecuentes, y los agentes planificadores probablemente fallar√°n. |
| 14B        | 12 GB VRAM (ej. RTX 3060) | ‚úÖ Usable para tareas simples. Puede tener dificultades con la navegaci√≥n web y tareas de planificaci√≥n. |
| 32B        | 24+ GB VRAM (ej. RTX 4090) | üöÄ √âxito con la mayor√≠a de las tareas, a√∫n puede tener dificultades con la planificaci√≥n de tareas |
| 70B+        | 48+ GB Vram (ej. mac studio) | üí™ Excelente. Recomendado para casos de uso avanzados. |

**P: ¬øPor qu√© Deepseek R1 sobre otros modelos?**  

Deepseek R1 sobresale en razonamiento y uso de herramientas para su tama√±o. Creemos que es una buena opci√≥n para nuestras necesidades, otros modelos funcionan bien, pero Deepseek es nuestra elecci√≥n principal.

**P: Obtengo un error al ejecutar `cli.py`. ¬øQu√© hago?**  

Aseg√∫rate de que local est√© ejecut√°ndose (`ollama serve`), tu `config.ini` coincida con tu proveedor, y las dependencias est√©n instaladas. Si nada funciona, no dudes en crear un issue.

**P: ¬øRealmente puede ejecutarse 100% localmente?**  

S√≠, con proveedores Ollama, lm-studio o server, todos los modelos de voz a texto, LLM y texto a voz se ejecutan localmente. Las opciones no locales (OpenAI u otras API) son opcionales.

**P: ¬øPor qu√© deber√≠a usar AgenticSeek cuando tengo Manus?**

Esto comenz√≥ como un Proyecto Secundario que hicimos por inter√©s en los agentes de IA. Lo especial es que queremos usar modelos locales y evitar APIs.
Nos inspiramos en Jarvis y Friday (pel√≠culas de Iron man) para hacerlo "genial" pero para la funcionalidad tomamos m√°s inspiraci√≥n de Manus, porque eso es lo que la gente quiere en primer lugar: una alternativa local a manus.
A diferencia de Manus, AgenticSeek prioriza la independencia de sistemas externos, d√°ndote m√°s control, privacidad y evitando costos de API.

## Contribuir

¬°Buscamos desarrolladores para mejorar AgenticSeek! Revisa los issues o discusiones abiertas.

[Gu√≠a de contribuci√≥n](./docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Mantenedores:

 > [Fosowl](https://github.com/Fosowl) | Hora de Par√≠s 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Hora de Taipei 

 > [steveh8758](https://github.com/steveh8758) | Hora de Taipei 

## Agradecimientos Especiales:

 > [tcsenpai](https://github.com/tcsenpai) y [plitc](https://github.com/plitc) Por ayudar con la dockerizaci√≥n del backend

